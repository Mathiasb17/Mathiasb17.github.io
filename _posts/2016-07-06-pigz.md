---
layout: post
title: "Parallel tar gzip"
comments: true
description: "Making .tar.gz archives really faster"
keywords: "tar, gz, .tar.gz, gzip, parallel algorithms"
---

To render simulation sequences i always need to make huge .tar.gz archives (~50GB) of .xml and .obj files, in order to upload them 
to the cluster computing the final sequence.

To do this i usually do :

```bash
tar czf my_archive.tar.gz my_folder
```

It takes ages on my lab computer (Intel Xeon E5620, 16GB DDR2 and Hard drive, no SSD).

[Pigz](http://zlib.net/pigz/) is a parallel implementation of gzip, and you can use it with the tar command as i discovered few
days ago.

As you can see, this is way faster using 8 cores :

```bash
du -h 07_res_front_mer 

56G     07_res_front_mer

time tar -czf 07_res_front_mer.tar.gz 07_res_front_mer/

3406.95user 192.53system 1:02:54elapsed 95%CPU

time tar -I pigz -cf 07_res_front_mer.tar.gz 07_res_front_mer/

3487.30user 136.83system 16:40.41elapsed 362%CPU
```

![pigz_htop](/images/pigz_htop.png)

Of course it is useless for small files/directories.

Enjoy !
